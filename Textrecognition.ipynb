{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "nYyInutqXpJN",
        "outputId": "d66ca35c-8335-472f-ce86-5c18f632916d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'content' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-2708476752.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcontent\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0miam\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mhandwriting\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'content' is not defined"
          ]
        }
      ],
      "source": [
        "/content/Dataset/iam-handwriting-word-database.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "InferenceModel"
      ],
      "metadata": {
        "id": "s8VEnjEpYx0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "# Set your Kaggle API credentials (you need to have a Kaggle account)\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"chirrashyamprakash\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"3ba910b04abfb4b38532c945fc8fa45a\"\n",
        "\n",
        "# Instantiate the Kaggle API\n",
        "api = KaggleApi()\n",
        "# Verify that the Kaggle API is authenticated\n",
        "api.authenticate()\n",
        "\n",
        "# Download the dataset\n",
        "dataset_name = \"nibinv23/iam-handwriting-word-database\"\n",
        "output_path = \"/content/Dataset\"  # Specify the path where you want to save the dataset\n",
        "\n",
        "# Download the dataset\n",
        "api.dataset_download_files(dataset_name, path=output_path, unzip=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "wR8cYW-MaGIL",
        "outputId": "ae9963af-1550-4022-8700-cc0bde44f3d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c392818f1a3a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkaggle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaggle_api_extended\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKaggleApi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Set your Kaggle API credentials (you need to have a Kaggle account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"KAGGLE_USERNAME\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"chirrashyamprakash\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kaggle/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKaggleApi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mApiClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\u001b[0m in \u001b[0;36mauthenticate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m                 raise IOError('Could not find {}. Make sure it\\'s located in'\n\u001b[0m\u001b[1;32m    399\u001b[0m                               ' {}. Or use the environment method.'.format(\n\u001b[1;32m    400\u001b[0m                                   self.config_file, self.config_dir))\n",
            "\u001b[0;31mOSError\u001b[0m: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "from tqdm import tqdm\n",
        "from urllib.request import urlopen\n",
        "from io import BytesIO\n",
        "from zipfile import ZipFile\n",
        "import os\n",
        "\n",
        "def download_and_unzip(url, extract_to='Datasets', chunk_size=1024*1024):\n",
        "    # Download the file\n",
        "    http_response = urlopen(url)\n",
        "\n",
        "    # Read the content in chunks\n",
        "    data = b''\n",
        "    iterations = http_response.length // chunk_size + 1\n",
        "    for _ in tqdm(range(iterations)):\n",
        "        data += http_response.read(chunk_size)\n",
        "\n",
        "    # Extract the zip file\n",
        "    with ZipFile(BytesIO(data)) as zipfile:\n",
        "        zipfile.extractall(path=extract_to)\n",
        "\n",
        "# Define the dataset path\n",
        "dataset_path = '/content/Dataset/IAM_Words'\n",
        "\n",
        "# Check if dataset exists, if not download and unzip\n",
        "if not os.path.exists(dataset_path):\n",
        "    # Download and unzip the dataset\n",
        "    download_and_unzip('https://git.io/J0fjL', extract_to='/content/Dataset')\n",
        "    # Extract words.tgz\n",
        "    with tarfile.open(os.path.join(dataset_path, \"words.tgz\")) as file:\n",
        "        file.extractall(path=dataset_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loGhZ22yadHK",
        "outputId": "fbf9bd21-4438-4cd3-be13-23241a576f27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 784/784 [05:40<00:00,  2.30it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/Dataset/IAM_Words'\n",
        "\n",
        "# Preprocess the dataset\n",
        "words_file_path = os.path.join(dataset_path, \"words.txt\")\n",
        "with open(words_file_path, \"r\") as words_file:\n",
        "    lines = words_file.readlines()\n",
        "\n",
        "for line in tqdm(lines):\n",
        "    if line.startswith(\"#\"):\n",
        "        continue\n",
        "\n",
        "    line_split = line.split(\" \")\n",
        "    if line_split[1] == \"err\":\n",
        "        continue\n",
        "\n",
        "    folder1 = line_split[0][:3]\n",
        "    folder2 = line_split[0][:8]\n",
        "    file_name = line_split[0] + \".png\"\n",
        "    label = line_split[-1].rstrip('\\n')\n",
        "\n",
        "    rel_path = os.path.join(dataset_path, \"words\", folder1, folder2, file_name)\n",
        "    if not os.path.exists(rel_path):\n",
        "        continue\n",
        "\n",
        "    dataset.append([rel_path, label])\n",
        "    vocab.update(list(label))\n",
        "    max_len = max(max_len, len(label))\n",
        "\n",
        "# Now you can continue with the rest of your code using `dataset`, `vocab`, and `max_len`.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxXTYS5wZGIH",
        "outputId": "7ecb0a44-7fea-412e-d70d-5a22b9c2ff9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 115338/115338 [00:01<00:00, 106746.92it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "\n",
        "class BaseModelConfigs:\n",
        "    def __init__(self):\n",
        "        self.model_path = None\n",
        "\n",
        "    def serialize(self):\n",
        "        class_attributes = {key: value\n",
        "                            for (key, value)\n",
        "                            in type(self).__dict__.items()\n",
        "                            if key not in ['__module__', '__init__', '__doc__', '__annotations__']}\n",
        "        instance_attributes = self.__dict__\n",
        "\n",
        "        # first init with class attributes then apply instance attributes overwriting any existing duplicate attributes\n",
        "        all_attributes = class_attributes.copy()\n",
        "        all_attributes.update(instance_attributes)\n",
        "\n",
        "        return all_attributes\n",
        "\n",
        "    def save(self, name: str = \"configs.yaml\"):\n",
        "        if self.model_path is None:\n",
        "            raise Exception(\"Model path is not specified\")\n",
        "\n",
        "        # create directory if not exist\n",
        "        if not os.path.exists(self.model_path):\n",
        "            os.makedirs(self.model_path)\n",
        "\n",
        "        with open(os.path.join(self.model_path, name), \"w\") as f:\n",
        "            yaml.dump(self.serialize(), f)\n",
        "\n",
        "    @staticmethod\n",
        "    def load(configs_path: str):\n",
        "        with open(configs_path, \"r\") as f:\n",
        "            configs = yaml.load(f, Loader=yaml.FullLoader)\n",
        "\n",
        "        config = BaseModelConfigs()\n",
        "        for key, value in configs.items():\n",
        "            setattr(config, key, value)\n",
        "\n",
        "        return config\n",
        "\n",
        "class ModelConfigs(BaseModelConfigs):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model_path = os.path.join(\"/content/extracted/202301111911\")\n",
        "        self.vocab = \"\"\n",
        "        self.height = 32\n",
        "        self.width = 128\n",
        "        self.max_text_length = 0\n",
        "        self.batch_size = 16\n",
        "        self.learning_rate = 0.0005\n",
        "        self.train_epochs = 1000\n",
        "        self.train_workers = 20"
      ],
      "metadata": {
        "id": "4WREFT_lZvsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a ModelConfigs object to store model configurations\n",
        "configs = ModelConfigs()\n",
        "\n",
        "# Save vocab and maximum text length to configs\n",
        "configs.vocab = \"\".join(vocab)\n",
        "configs.max_text_length = max_len\n",
        "configs.save()"
      ],
      "metadata": {
        "id": "sDA5CkQKZyLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Path to the zip file\n",
        "zip_file_path = \"/content/202301111911-20240515T134220Z-001.zip\"\n",
        "\n",
        "# Directory where you want to extract the files\n",
        "extracted_dir = \"/content/extracted/\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs(extracted_dir, exist_ok=True)\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_dir)\n",
        "\n",
        "print(\"Zip file has been extracted to:\", extracted_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xy4Lsxcnf2C6",
        "outputId": "ee152b48-a425-4729-cdde-1cf0b8e53d67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zip file has been extracted to: /content/extracted/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import typing\n",
        "import numpy as np\n",
        "import onnxruntime as ort\n",
        "from collections import deque\n",
        "\n",
        "class FpsWrapper:\n",
        "    \"\"\" Decorator to calculate the frames per second of a function\n",
        "    \"\"\"\n",
        "    def __init__(self, func: typing.Callable):\n",
        "        self.func = func\n",
        "        self.fps_list = deque([], maxlen=100)\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        start = time.time()\n",
        "        results = self.func(self.instance, *args, **kwargs)\n",
        "        self.fps_list.append(1 / (time.time() - start))\n",
        "        self.instance.fps = np.mean(self.fps_list)\n",
        "        return results\n",
        "\n",
        "    def __get__(self, instance, owner):\n",
        "        self.instance = instance\n",
        "        return self.__call__.__get__(instance, owner)\n",
        "\n",
        "\n",
        "class OnnxInferenceModel:\n",
        "    \"\"\" Base class for all inference models that use onnxruntime\n",
        "\n",
        "    Attributes:\n",
        "        model_path (str, optional): Path to the model folder. Defaults to \"\".\n",
        "        force_cpu (bool, optional): Force the model to run on CPU or GPU. Defaults to GPU.\n",
        "        default_model_name (str, optional): Default model name. Defaults to \"model.onnx\".\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path: str = \"\",\n",
        "        force_cpu: bool = False,\n",
        "        default_model_name: str = \"model.onnx\",\n",
        "        *args, **kwargs\n",
        "        ):\n",
        "        self.model_path = model_path.replace(\"\\\\\", \"/\")\n",
        "        self.force_cpu = force_cpu\n",
        "        self.default_model_name = default_model_name\n",
        "\n",
        "        # check if model path is a directory with os path\n",
        "        if os.path.isdir(self.model_path):\n",
        "            self.model_path = os.path.join(self.model_path, self.default_model_name)\n",
        "\n",
        "        if not os.path.exists(self.model_path):\n",
        "            raise Exception(f\"Model path ({self.model_path}) does not exist\")\n",
        "\n",
        "        providers = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"] if ort.get_device() == \"GPU\" and not force_cpu else [\"CPUExecutionProvider\"]\n",
        "\n",
        "        self.model = ort.InferenceSession(self.model_path, providers=providers)\n",
        "\n",
        "        self.metadata = {}\n",
        "        if self.model.get_modelmeta().custom_metadata_map:\n",
        "            # add metadata to self object\n",
        "            for key, value in self.model.get_modelmeta().custom_metadata_map.items():\n",
        "                try:\n",
        "                    new_value = eval(value) # in case the value is a list or dict\n",
        "                except:\n",
        "                    new_value = value\n",
        "                self.metadata[key] = new_value\n",
        "\n",
        "        # Update providers priority to only CPUExecutionProvider\n",
        "        if self.force_cpu:\n",
        "            self.model.set_providers([\"CPUExecutionProvider\"])\n",
        "\n",
        "        self.input_shapes = [meta.shape for meta in self.model.get_inputs()]\n",
        "        self.input_names = [meta.name for meta in self.model._inputs_meta]\n",
        "        self.output_names = [meta.name for meta in self.model._outputs_meta]\n",
        "\n",
        "    def predict(self, data: np.ndarray, *args, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @FpsWrapper\n",
        "    def __call__(self, data: np.ndarray):\n",
        "        results = self.predict(data)\n",
        "        return results"
      ],
      "metadata": {
        "id": "eLKO2GPEkgWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import typing\n",
        "import numpy as np\n",
        "from itertools import groupby\n",
        "\n",
        "\n",
        "def ctc_decoder(predictions: np.ndarray, chars: typing.Union[str, list]) -> typing.List[str]:\n",
        "    \"\"\" CTC greedy decoder for predictions\n",
        "\n",
        "    Args:\n",
        "        predictions (np.ndarray): predictions from model\n",
        "        chars (typing.Union[str, list]): list of characters\n",
        "\n",
        "    Returns:\n",
        "        typing.List[str]: list of words\n",
        "    \"\"\"\n",
        "    # use argmax to find the index of the highest probability\n",
        "    argmax_preds = np.argmax(predictions, axis=-1)\n",
        "\n",
        "    # use groupby to find continuous same indexes\n",
        "    grouped_preds = [[k for k,_ in groupby(preds)] for preds in argmax_preds]\n",
        "\n",
        "    # convert indexes to chars\n",
        "    texts = [\"\".join([chars[k] for k in group if k < len(chars)]) for group in grouped_preds]\n",
        "\n",
        "    return texts\n",
        "\n",
        "\n",
        "def edit_distance(prediction_tokens: typing.List[str], reference_tokens: typing.List[str]) -> int:\n",
        "    \"\"\" Standard dynamic programming algorithm to compute the Levenshtein Edit Distance Algorithm\n",
        "\n",
        "    Args:\n",
        "        prediction_tokens: A tokenized predicted sentence\n",
        "        reference_tokens: A tokenized reference sentence\n",
        "    Returns:\n",
        "        Edit distance between the predicted sentence and the reference sentence\n",
        "    \"\"\"\n",
        "    # Initialize a matrix to store the edit distances\n",
        "    dp = [[0] * (len(reference_tokens) + 1) for _ in range(len(prediction_tokens) + 1)]\n",
        "\n",
        "    # Fill the first row and column with the number of insertions needed\n",
        "    for i in range(len(prediction_tokens) + 1):\n",
        "        dp[i][0] = i\n",
        "\n",
        "    for j in range(len(reference_tokens) + 1):\n",
        "        dp[0][j] = j\n",
        "\n",
        "    # Iterate through the prediction and reference tokens\n",
        "    for i, p_tok in enumerate(prediction_tokens):\n",
        "        for j, r_tok in enumerate(reference_tokens):\n",
        "            # If the tokens are the same, the edit distance is the same as the previous entry\n",
        "            if p_tok == r_tok:\n",
        "                dp[i+1][j+1] = dp[i][j]\n",
        "            # If the tokens are different, the edit distance is the minimum of the previous entries plus 1\n",
        "            else:\n",
        "                dp[i+1][j+1] = min(dp[i][j+1], dp[i+1][j], dp[i][j]) + 1\n",
        "\n",
        "    # Return the final entry in the matrix as the edit distance\n",
        "    return dp[-1][-1]\n",
        "\n",
        "def get_cer(\n",
        "    preds: typing.Union[str, typing.List[str]],\n",
        "    target: typing.Union[str, typing.List[str]],\n",
        "    ) -> float:\n",
        "    \"\"\" Update the cer score with the current set of references and predictions.\n",
        "\n",
        "    Args:\n",
        "        preds (typing.Union[str, typing.List[str]]): list of predicted sentences\n",
        "        target (typing.Union[str, typing.List[str]]): list of target words\n",
        "\n",
        "    Returns:\n",
        "        Character error rate score\n",
        "    \"\"\"\n",
        "    if isinstance(preds, str):\n",
        "        preds = [preds]\n",
        "    if isinstance(target, str):\n",
        "        target = [target]\n",
        "\n",
        "    total, errors = 0, 0\n",
        "    for pred_tokens, tgt_tokens in zip(preds, target):\n",
        "        errors += edit_distance(list(pred_tokens), list(tgt_tokens))\n",
        "        total += len(tgt_tokens)\n",
        "\n",
        "    if total == 0:\n",
        "        return 0.0\n",
        "\n",
        "    cer = errors / total\n",
        "\n",
        "    return cer\n",
        "\n",
        "def get_wer(\n",
        "    preds: typing.Union[str, typing.List[str]],\n",
        "    target: typing.Union[str, typing.List[str]],\n",
        "    ) -> float:\n",
        "    \"\"\" Update the wer score with the current set of references and predictions.\n",
        "\n",
        "    Args:\n",
        "        target (typing.Union[str, typing.List[str]]): string of target sentence or list of target words\n",
        "        preds (typing.Union[str, typing.List[str]]): string of predicted sentence or list of predicted words\n",
        "\n",
        "    Returns:\n",
        "        Word error rate score\n",
        "    \"\"\"\n",
        "    if isinstance(preds, str) and isinstance(target, str):\n",
        "        preds = [preds]\n",
        "        target = [target]\n",
        "\n",
        "    if isinstance(preds, list) and isinstance(target, list):\n",
        "        errors, total_words = 0, 0\n",
        "        for _pred, _target in zip(preds, target):\n",
        "            if isinstance(_pred, str) and isinstance(_target, str):\n",
        "                errors += edit_distance(_pred.split(), _target.split())\n",
        "                total_words += len(_target.split())\n",
        "            else:\n",
        "                print(\"Error: preds and target must be either both strings or both lists of strings.\")\n",
        "                return np.inf\n",
        "\n",
        "    else:\n",
        "        print(\"Error: preds and target must be either both strings or both lists of strings.\")\n",
        "        return np.inf\n",
        "\n",
        "    wer = errors / total_words\n",
        "\n",
        "    return wer"
      ],
      "metadata": {
        "id": "Ty4riK2Ckwc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inferenceModel.py\n",
        "import cv2\n",
        "import typing\n",
        "import numpy as np\n",
        "\n",
        "class ImageToWordModel(OnnxInferenceModel):\n",
        "    def __init__(self, char_list: typing.Union[str, list], *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.char_list = char_list\n",
        "\n",
        "    def predict(self, image: np.ndarray):\n",
        "        image = cv2.resize(image, self.input_shapeS[:2][::-1])\n",
        "\n",
        "        image_pred = np.expand_dims(image, axis=0).astype(np.float32)\n",
        "\n",
        "        preds = self.model.run(None, {self.input_name: image_pred})[0]\n",
        "\n",
        "        text = ctc_decoder(preds, self.char_list)[0]\n",
        "\n",
        "        return text\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import pandas as pd\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    configs = BaseModelConfigs.load(\"/content/extracted/202301111911/configs.yaml\")\n",
        "\n",
        "    model = ImageToWordModel(model_path=configs.model_path, char_list=configs.vocab)\n",
        "\n",
        "    df = pd.read_csv(\"/content/extracted/202301111911/val.csv\").values.tolist()\n",
        "\n",
        "    accum_cer = []\n",
        "    for image_path, label in tqdm(df):\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        prediction_text = model.predict(image)\n",
        "\n",
        "        cer = get_cer(prediction_text, label)\n",
        "        print(f\"Image: {image_path}, Label: {label}, Prediction: {prediction_text}, CER: {cer}\")\n",
        "\n",
        "        accum_cer.append(cer)\n",
        "\n",
        "    print(f\"Average CER: {np.average(accum_cer)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "rUHycr8heH7u",
        "outputId": "2ad0658d-0e6e-45e6-beef-f3a24968e815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/2171 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'ImageToWordModel' object has no attribute 'input_shapeS'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-1c85ec103615>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mprediction_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mcer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-1c85ec103615>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shapeS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mimage_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ImageToWordModel' object has no attribute 'input_shapeS'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEAB-aT5pmx1",
        "outputId": "338f45f8-1727-410b-cabe-6b326a476eef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "\n",
        "# Load the ONNX model\n",
        "model_path = \"/content/extracted/202301111911/model.onnx\"\n",
        "model = onnx.load(model_path)\n",
        "\n",
        "# Print model summary\n",
        "print(onnx.helper.printable_graph(model.graph))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdgSLscAewvZ",
        "outputId": "76f57ccd-6477-4b6e-c1a6-7afe08933cca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "graph tf2onnx (\n",
            "  %input[FLOAT, unk__438x32x128x3]\n",
            ") initializers (\n",
            "  %model/output/Tensordot/free:0[INT32, 2]\n",
            "  %model/output/Tensordot/axes:0[INT32, 1]\n",
            "  %model/output/Tensordot/ReadVariableOp:0[FLOAT, 256x75]\n",
            "  %model/output/Tensordot/Const_2:0[INT32, 1]\n",
            "  %model/output/BiasAdd/ReadVariableOp:0[FLOAT, 75]\n",
            "  %model/conv2d_9/BiasAdd_weights_fused_bn[FLOAT, 32x32x3x3]\n",
            "  %model/conv2d_9/BiasAdd_bias_fused_bn[FLOAT, 32]\n",
            "  %model/conv2d_8/BiasAdd_weights_fused_bn[FLOAT, 32x16x3x3]\n",
            "  %model/conv2d_8/BiasAdd_bias_fused_bn[FLOAT, 32]\n",
            "  %model/conv2d_7/BiasAdd_weights_fused_bn[FLOAT, 16x16x3x3]\n",
            "  %model/conv2d_7/BiasAdd_bias_fused_bn[FLOAT, 16]\n",
            "  %model/conv2d_6/BiasAdd_weights_fused_bn[FLOAT, 16x16x3x3]\n",
            "  %model/conv2d_6/BiasAdd_bias_fused_bn[FLOAT, 16]\n",
            "  %model/conv2d_5/Conv2D/ReadVariableOp:0[FLOAT, 16x16x1x1]\n",
            "  %model/conv2d_5/BiasAdd/ReadVariableOp:0[FLOAT, 16]\n",
            "  %model/conv2d_4/BiasAdd_weights_fused_bn[FLOAT, 16x16x3x3]\n",
            "  %model/conv2d_4/BiasAdd_bias_fused_bn[FLOAT, 16]\n",
            "  %model/conv2d_3/BiasAdd_weights_fused_bn[FLOAT, 16x16x3x3]\n",
            "  %model/conv2d_3/BiasAdd_bias_fused_bn[FLOAT, 16]\n",
            "  %model/conv2d_22/BiasAdd_weights_fused_bn[FLOAT, 64x64x3x3]\n",
            "  %model/conv2d_22/BiasAdd_bias_fused_bn[FLOAT, 64]\n",
            "  %model/conv2d_21/BiasAdd_weights_fused_bn[FLOAT, 64x64x3x3]\n",
            "  %model/conv2d_21/BiasAdd_bias_fused_bn[FLOAT, 64]\n",
            "  %model/conv2d_20/BiasAdd_weights_fused_bn[FLOAT, 64x64x3x3]\n",
            "  %model/conv2d_20/BiasAdd_bias_fused_bn[FLOAT, 64]\n",
            "  %model/conv2d_2/Conv2D/ReadVariableOp:0[FLOAT, 16x3x1x1]\n",
            "  %model/conv2d_2/BiasAdd/ReadVariableOp:0[FLOAT, 16]\n",
            "  %model/conv2d_19/BiasAdd_weights_fused_bn[FLOAT, 64x64x3x3]\n",
            "  %model/conv2d_19/BiasAdd_bias_fused_bn[FLOAT, 64]\n",
            "  %model/conv2d_18/Conv2D/ReadVariableOp:0[FLOAT, 64x64x1x1]\n",
            "  %model/conv2d_18/BiasAdd/ReadVariableOp:0[FLOAT, 64]\n",
            "  %model/conv2d_17/BiasAdd_weights_fused_bn[FLOAT, 64x64x3x3]\n",
            "  %model/conv2d_17/BiasAdd_bias_fused_bn[FLOAT, 64]\n",
            "  %model/conv2d_16/BiasAdd_weights_fused_bn[FLOAT, 64x64x3x3]\n",
            "  %model/conv2d_16/BiasAdd_bias_fused_bn[FLOAT, 64]\n",
            "  %model/conv2d_15/Conv2D/ReadVariableOp:0[FLOAT, 64x32x1x1]\n",
            "  %model/conv2d_15/BiasAdd/ReadVariableOp:0[FLOAT, 64]\n",
            "  %model/conv2d_14/BiasAdd_weights_fused_bn[FLOAT, 64x64x3x3]\n",
            "  %model/conv2d_14/BiasAdd_bias_fused_bn[FLOAT, 64]\n",
            "  %model/conv2d_13/BiasAdd_weights_fused_bn[FLOAT, 64x32x3x3]\n",
            "  %model/conv2d_13/BiasAdd_bias_fused_bn[FLOAT, 64]\n",
            "  %model/conv2d_12/BiasAdd_weights_fused_bn[FLOAT, 32x32x3x3]\n",
            "  %model/conv2d_12/BiasAdd_bias_fused_bn[FLOAT, 32]\n",
            "  %model/conv2d_11/BiasAdd_weights_fused_bn[FLOAT, 32x32x3x3]\n",
            "  %model/conv2d_11/BiasAdd_bias_fused_bn[FLOAT, 32]\n",
            "  %model/conv2d_10/Conv2D/ReadVariableOp:0[FLOAT, 32x16x1x1]\n",
            "  %model/conv2d_10/BiasAdd/ReadVariableOp:0[FLOAT, 32]\n",
            "  %model/conv2d_1/BiasAdd_weights_fused_bn[FLOAT, 16x16x3x3]\n",
            "  %model/conv2d_1/BiasAdd_bias_fused_bn[FLOAT, 16]\n",
            "  %model/conv2d/BiasAdd_weights_fused_bn[FLOAT, 16x3x3x3]\n",
            "  %model/conv2d/BiasAdd_bias_fused_bn[FLOAT, 16]\n",
            "  %model/bidirectional/backward_lstm/zeros_1/Const:0[FLOAT, scalar]\n",
            "  %const_starts__96[INT64, 1]\n",
            "  %const_fold_opt__437[INT32, 1]\n",
            "  %const_fold_opt__433[INT32, 1]\n",
            "  %const_ends__97[INT64, 1]\n",
            "  %const_axes__309[INT64, 1]\n",
            "  %W__84[FLOAT, 2x512x64]\n",
            "  %R__85[FLOAT, 2x512x128]\n",
            "  %ConstantFolding/model/lambda/truediv_recip:0[FLOAT, scalar]\n",
            "  %Const__430[INT64, 4]\n",
            "  %Const__426[INT64, 3]\n",
            "  %B__86[FLOAT, 2x1024]\n",
            ") {\n",
            "  %model/lambda/truediv:0 = Mul(%input, %ConstantFolding/model/lambda/truediv_recip:0)\n",
            "  %model/conv2d_2/BiasAdd__101:0 = Transpose[perm = [0, 3, 1, 2]](%model/lambda/truediv:0)\n",
            "  %model/conv2d_2/BiasAdd:0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%model/conv2d_2/BiasAdd__101:0, %model/conv2d_2/Conv2D/ReadVariableOp:0, %model/conv2d_2/BiasAdd/ReadVariableOp:0)\n",
            "  %model/batch_normalization/FusedBatchNormV3:0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%model/conv2d_2/BiasAdd__101:0, %model/conv2d/BiasAdd_weights_fused_bn, %model/conv2d/BiasAdd_bias_fused_bn)\n",
            "  %model/leaky_re_lu/LeakyRelu:0 = LeakyRelu[alpha = 0.100000001490116](%model/batch_normalization/FusedBatchNormV3:0)\n",
            "  %model/batch_normalization_1/FusedBatchNormV3:0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%model/leaky_re_lu/LeakyRelu:0, %model/conv2d_1/BiasAdd_weights_fused_bn, %model/conv2d_1/BiasAdd_bias_fused_bn)\n",
            "  %model/add/add:0 = Add(%model/batch_normalization_1/FusedBatchNormV3:0, %model/conv2d_2/BiasAdd:0)\n",
            "  %model/leaky_re_lu_1/LeakyRelu:0 = LeakyRelu[alpha = 0.100000001490116](%model/add/add:0)\n",
            "  %model/conv2d_5/BiasAdd:0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [2, 2]](%model/leaky_re_lu_1/LeakyRelu:0, %model/conv2d_5/Conv2D/ReadVariableOp:0, %model/conv2d_5/BiasAdd/ReadVariableOp:0)\n",
            "  %model/batch_normalization_2/FusedBatchNormV3:0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [0, 0, 1, 1], strides = [2, 2]](%model/leaky_re_lu_1/LeakyRelu:0, %model/conv2d_3/BiasAdd_weights_fused_bn, %model/conv2d_3/BiasAdd_bias_fused_bn)\n",
            "  %model/leaky_re_lu_2/LeakyRelu:0 = LeakyRelu[alpha = 0.100000001490116](%model/batch_normalization_2/FusedBatchNormV3:0)\n",
            "  %model/batch_normalization_3/FusedBatchNormV3:0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%model/leaky_re_lu_2/LeakyRelu:0, %model/conv2d_4/BiasAdd_weights_fused_bn, %model/conv2d_4/BiasAdd_bias_fused_bn)\n",
            "  %model/add_1/add:0 = Add(%model/batch_normalization_3/FusedBatchNormV3:0, %model/conv2d_5/BiasAdd:0)\n",
            "  %model/leaky_re_lu_3/LeakyRelu:0 = LeakyRelu[alpha = 0.100000001490116](%model/add_1/add:0)\n",
            "  %model/batch_normalization_4/FusedBatchNormV3:0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%model/leaky_re_lu_3/LeakyRelu:0, %model/conv2d_6/BiasAdd_weights_fused_bn, %model/conv2d_6/BiasAdd_bias_fused_bn)\n",
            "  %model/leaky_re_lu_4/LeakyRelu:0 = LeakyRelu[alpha = 0.100000001490116](%model/batch_normalization_4/FusedBatchNormV3:0)\n",
            "  %model/batch_normalization_5/FusedBatchNormV3:0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%model/leaky_re_lu_4/LeakyRelu:0, %model/conv2d_7/BiasAdd_weights_fused_bn, %model/conv2d_7/BiasAdd_bias_fused_bn)\n",
            "  %model/add_2/add:0 = Add(%model/batch_normalization_5/FusedBatchNormV3:0, %model/leaky_re_lu_3/LeakyRelu:0)\n",
            "  %model/leaky_re_lu_5/LeakyRelu:0 = LeakyRelu[alpha = 0.100000001490116](%model/add_2/add:0)\n",
            "  %model/batch_normalization_6/FusedBatchNormV3:0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [0, 0, 1, 1], strides = [2, 2]](%model/leaky_re_lu_5/LeakyRelu:0, %model/conv2d_8/BiasAdd_weights_fused_bn, %model/conv2d_8/BiasAdd_bias_fused_bn)\n",
            "  %model/leaky_re_lu_6/LeakyRelu:0 = LeakyRelu[alpha = 0.100000001490116](%model/batch_normalization_6/FusedBatchNormV3:0)\n",
            "  %model/batch_normalization_7/FusedBatchNormV3:0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%model/leaky_re_lu_6/LeakyRelu:0, %model/conv2d_9/BiasAdd_weights_fused_bn, %model/conv2d_9/BiasAdd_bias_fused_bn)\n",
            "  %model/conv2d_10/BiasAdd:0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [2, 2]](%model/leaky_re_lu_5/LeakyRelu:0, %model/conv2d_10/Conv2D/ReadVariableOp:0, %model/conv2d_10/BiasAdd/ReadVariableOp:0)\n",
            "  %model/add_3/add:0 = Add(%model/batch_normalization_7/FusedBatchNormV3:0, %model/conv2d_10/BiasAdd:0)\n",
            "  %model/leaky_re_lu_7/LeakyRelu:0 = LeakyRelu[alpha = 0.100000001490116](%model/add_3/add:0)\n",
            "  %model/batch_normalization_8/FusedBatchNormV3:0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%model/leaky_re_lu_7/LeakyRelu:0, %model/conv2d_11/BiasAdd_weights_fused_bn, %model/conv2d_11/BiasAdd_bias_fused_bn)\n",
            "  %model/leaky_re_lu_8/LeakyRelu:0 = LeakyRelu[alpha = 0.100000001490116](%model/batch_normalization_8/FusedBatchNormV3:0)\n",
            "  %model/batch_normalization_9/FusedBatchNormV3:0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%model/leaky_re_lu_8/LeakyRelu:0, %model/conv2d_12/BiasAdd_weights_fused_bn, %model/conv2d_12/BiasAdd_bias_fused_bn)\n",
            "  %model/add_4/add:0 = Add(%model/batch_normalization_9/FusedBatchNormV3:0, %model/leaky_re_lu_7/LeakyRelu:0)\n",
            "  %model/leaky_re_lu_9/LeakyRelu:0 = LeakyRelu[alpha = 0.100000001490116](%model/add_4/add:0)\n",
            "  %model/conv2d_15/BiasAdd:0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [2, 2]](%model/leaky_re_lu_9/LeakyRelu:0, %model/conv2d_15/Conv2D/ReadVariableOp:0, %model/conv2d_15/BiasAdd/ReadVariableOp:0)\n",
            "  %model/batch_normalization_10/FusedBatchNormV3:0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [0, 0, 1, 1], strides = [2, 2]](%model/leaky_re_lu_9/LeakyRelu:0, %model/conv2d_13/BiasAdd_weights_fused_bn, %model/conv2d_13/BiasAdd_bias_fused_bn)\n",
            "  %model/leaky_re_lu_10/LeakyRelu:0 = LeakyRelu[alpha = 0.100000001490116](%model/batch_normalization_10/FusedBatchNormV3:0)\n",
            "  %model/batch_normalization_11/FusedBatchNormV3:0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%model/leaky_re_lu_10/LeakyRelu:0, %model/conv2d_14/BiasAdd_weights_fused_bn, %model/conv2d_14/BiasAdd_bias_fused_bn)\n",
            "  %model/add_5/add:0 = Add(%model/batch_normalization_11/FusedBatchNormV3:0, %model/conv2d_15/BiasAdd:0)\n",
            "  %model/leaky_re_lu_11/LeakyRelu:0 = LeakyRelu[alpha = 0.100000001490116](%model/add_5/add:0)\n",
            "  %model/conv2d_18/BiasAdd:0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%model/leaky_re_lu_11/LeakyRelu:0, %model/conv2d_18/Conv2D/ReadVariableOp:0, %model/conv2d_18/BiasAdd/ReadVariableOp:0)\n",
            "  %model/batch_normalization_12/FusedBatchNormV3:0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%model/leaky_re_lu_11/LeakyRelu:0, %model/conv2d_16/BiasAdd_weights_fused_bn, %model/conv2d_16/BiasAdd_bias_fused_bn)\n",
            "  %model/leaky_re_lu_12/LeakyRelu:0 = LeakyRelu[alpha = 0.100000001490116](%model/batch_normalization_12/FusedBatchNormV3:0)\n",
            "  %model/batch_normalization_13/FusedBatchNormV3:0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%model/leaky_re_lu_12/LeakyRelu:0, %model/conv2d_17/BiasAdd_weights_fused_bn, %model/conv2d_17/BiasAdd_bias_fused_bn)\n",
            "  %model/add_6/add:0 = Add(%model/batch_normalization_13/FusedBatchNormV3:0, %model/conv2d_18/BiasAdd:0)\n",
            "  %model/leaky_re_lu_13/LeakyRelu:0 = LeakyRelu[alpha = 0.100000001490116](%model/add_6/add:0)\n",
            "  %model/batch_normalization_14/FusedBatchNormV3:0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%model/leaky_re_lu_13/LeakyRelu:0, %model/conv2d_19/BiasAdd_weights_fused_bn, %model/conv2d_19/BiasAdd_bias_fused_bn)\n",
            "  %model/leaky_re_lu_14/LeakyRelu:0 = LeakyRelu[alpha = 0.100000001490116](%model/batch_normalization_14/FusedBatchNormV3:0)\n",
            "  %model/batch_normalization_15/FusedBatchNormV3:0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%model/leaky_re_lu_14/LeakyRelu:0, %model/conv2d_20/BiasAdd_weights_fused_bn, %model/conv2d_20/BiasAdd_bias_fused_bn)\n",
            "  %model/add_7/add:0 = Add(%model/batch_normalization_15/FusedBatchNormV3:0, %model/leaky_re_lu_13/LeakyRelu:0)\n",
            "  %model/leaky_re_lu_15/LeakyRelu:0 = LeakyRelu[alpha = 0.100000001490116](%model/add_7/add:0)\n",
            "  %model/batch_normalization_16/FusedBatchNormV3:0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%model/leaky_re_lu_15/LeakyRelu:0, %model/conv2d_21/BiasAdd_weights_fused_bn, %model/conv2d_21/BiasAdd_bias_fused_bn)\n",
            "  %model/leaky_re_lu_16/LeakyRelu:0 = LeakyRelu[alpha = 0.100000001490116](%model/batch_normalization_16/FusedBatchNormV3:0)\n",
            "  %model/batch_normalization_17/FusedBatchNormV3:0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%model/leaky_re_lu_16/LeakyRelu:0, %model/conv2d_22/BiasAdd_weights_fused_bn, %model/conv2d_22/BiasAdd_bias_fused_bn)\n",
            "  %model/add_8/add:0 = Add(%model/batch_normalization_17/FusedBatchNormV3:0, %model/leaky_re_lu_15/LeakyRelu:0)\n",
            "  %model/leaky_re_lu_17/LeakyRelu:0 = LeakyRelu[alpha = 0.100000001490116](%model/add_8/add:0)\n",
            "  %Transpose__419:0 = Transpose[perm = [0, 2, 3, 1]](%model/leaky_re_lu_17/LeakyRelu:0)\n",
            "  %Shape__428:0 = Shape(%model/leaky_re_lu_17/LeakyRelu:0)\n",
            "  %model/reshape/Shape:0 = Gather(%Shape__428:0, %Const__430)\n",
            "  %model/reshape/Shape__265:0 = Cast[to = 6](%model/reshape/Shape:0)\n",
            "  %model/reshape/strided_slice:0 = Slice(%model/reshape/Shape__265:0, %const_axes__309, %const_starts__96, %const_axes__309)\n",
            "  %model/reshape/Reshape/shape_Concat__277:0 = Concat[axis = 0](%model/reshape/strided_slice:0, %const_fold_opt__437, %const_fold_opt__437)\n",
            "  %model/reshape/Reshape__278:0 = Cast[to = 7](%model/reshape/Reshape/shape_Concat__277:0)\n",
            "  %model/reshape/Reshape:0 = Reshape(%Transpose__419:0, %model/reshape/Reshape__278:0)\n",
            "  %model/bidirectional/forward_lstm/Shape:0 = Shape(%model/reshape/Reshape:0)\n",
            "  %model/bidirectional/forward_lstm/Shape__279:0 = Cast[to = 6](%model/bidirectional/forward_lstm/Shape:0)\n",
            "  %model/bidirectional/forward_lstm/strided_slice:0 = Slice(%model/bidirectional/forward_lstm/Shape__279:0, %const_axes__309, %const_starts__96, %const_axes__309)\n",
            "  %model/bidirectional/forward_lstm/zeros/packed_Concat__295:0 = Concat[axis = 0](%model/bidirectional/forward_lstm/strided_slice:0, %const_fold_opt__433)\n",
            "  %model/bidirectional/backward_lstm/zeros_1__308:0 = Cast[to = 7](%model/bidirectional/forward_lstm/zeros/packed_Concat__295:0)\n",
            "  %model/bidirectional/forward_lstm/zeros:0 = Expand(%model/bidirectional/backward_lstm/zeros_1/Const:0, %model/bidirectional/backward_lstm/zeros_1__308:0)\n",
            "  %Unsqueeze__58:0 = Unsqueeze(%model/bidirectional/forward_lstm/zeros:0, %const_axes__309)\n",
            "  %model/bidirectional/forward_lstm/PartitionedCall/transpose:0 = Transpose[perm = [1, 0, 2]](%model/reshape/Reshape:0)\n",
            "  %Concat__78:0 = Concat[axis = 0](%Unsqueeze__58:0, %Unsqueeze__58:0)\n",
            "  %LSTM__87:0, %LSTM__87:1, %LSTM__87:2 = LSTM[direction = 'bidirectional', hidden_size = 128](%model/bidirectional/forward_lstm/PartitionedCall/transpose:0, %W__84, %R__85, %B__86, %, %Concat__78:0, %Concat__78:0)\n",
            "  %Slice__99:0 = Slice(%LSTM__87:0, %const_starts__96, %const_ends__97, %const_starts__96)\n",
            "  %Squeeze__70:0 = Squeeze(%Slice__99:0, %const_starts__96)\n",
            "  %Slice__94:0 = Slice(%LSTM__87:0, %const_axes__309, %const_starts__96, %const_starts__96)\n",
            "  %Squeeze__49:0 = Squeeze(%Slice__94:0, %const_starts__96)\n",
            "  %model/bidirectional/concat:0 = Concat[axis = 2](%Squeeze__49:0, %Squeeze__70:0)\n",
            "  %Transpose__384:0 = Transpose[perm = [1, 0, 2]](%model/bidirectional/concat:0)\n",
            "  %Shape__423:0 = Shape(%model/bidirectional/concat:0)\n",
            "  %model/output/Tensordot/Shape:0 = Gather(%Shape__423:0, %Const__426)\n",
            "  %model/output/Tensordot/Shape__315:0 = Cast[to = 6](%model/output/Tensordot/Shape:0)\n",
            "  %model/output/Tensordot/GatherV2_1:0 = Gather[axis = 0](%model/output/Tensordot/Shape__315:0, %model/output/Tensordot/axes:0)\n",
            "  %model/output/Tensordot/Prod_1:0 = ReduceProd[axes = [0], keepdims = 0](%model/output/Tensordot/GatherV2_1:0)\n",
            "  %model/output/Tensordot/stack_Unsqueeze__319:0 = Unsqueeze(%model/output/Tensordot/Prod_1:0, %const_axes__309)\n",
            "  %model/output/Tensordot/GatherV2:0 = Gather[axis = 0](%model/output/Tensordot/Shape__315:0, %model/output/Tensordot/free:0)\n",
            "  %model/output/Tensordot/concat_1:0 = Concat[axis = 0](%model/output/Tensordot/GatherV2:0, %model/output/Tensordot/Const_2:0)\n",
            "  %model/output/Tensordot__322:0 = Cast[to = 7](%model/output/Tensordot/concat_1:0)\n",
            "  %model/output/Tensordot/Prod:0 = ReduceProd[axes = [0], keepdims = 0](%model/output/Tensordot/GatherV2:0)\n",
            "  %model/output/Tensordot/stack_Unsqueeze__317:0 = Unsqueeze(%model/output/Tensordot/Prod:0, %const_axes__309)\n",
            "  %model/output/Tensordot/stack_Concat__320:0 = Concat[axis = 0](%model/output/Tensordot/stack_Unsqueeze__317:0, %model/output/Tensordot/stack_Unsqueeze__319:0)\n",
            "  %model/output/Tensordot/Reshape__321:0 = Cast[to = 7](%model/output/Tensordot/stack_Concat__320:0)\n",
            "  %model/output/Tensordot/Reshape:0 = Reshape(%Transpose__384:0, %model/output/Tensordot/Reshape__321:0)\n",
            "  %model/output/Tensordot/MatMul:0 = MatMul(%model/output/Tensordot/Reshape:0, %model/output/Tensordot/ReadVariableOp:0)\n",
            "  %model/output/Tensordot:0 = Reshape(%model/output/Tensordot/MatMul:0, %model/output/Tensordot__322:0)\n",
            "  %model/output/BiasAdd:0 = Add(%model/output/Tensordot:0, %model/output/BiasAdd/ReadVariableOp:0)\n",
            "  %output = Softmax(%model/output/BiasAdd:0)\n",
            "  return %output\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Load the ONNX model\n",
        "model_path = '/content/extracted/202301111911/model.onnx'\n",
        "session = onnxruntime.InferenceSession(model_path)\n",
        "\n",
        "# Preprocess the input image\n",
        "def preprocess_image(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize((128, 32)),  # Resize to match model input shape\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    image = preprocess(image)\n",
        "    image = image.numpy()\n",
        "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
        "    return image\n",
        "\n",
        "# Make predictions\n",
        "def predict(image_path):\n",
        "    input_image = preprocess_image(image_path)\n",
        "    input_name = session.get_inputs()[0].name\n",
        "    output_name = session.get_outputs()[0].name\n",
        "    pred = session.run([output_name], {input_name: input_image})\n",
        "    return pred\n",
        "\n",
        "# Example usage\n",
        "image_path = '/content/Dataset/iam_words/words/a01/a01-000u/a01-000u-00-00.png'\n",
        "prediction = predict(image_path)\n",
        "print(prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "aFUAQj2spHkf",
        "outputId": "bd2279fb-3c16-4248-ec26-14174dec925b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgument",
          "evalue": "[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: input for the following indices\n index: 1 Got: 3 Expected: 32\n index: 3 Got: 32 Expected: 3\n Please fix either the inputs/outputs or the model.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-a307732bc28f>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/Dataset/iam_words/words/a01/a01-000u/a01-000u-00-00.png'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-a307732bc28f>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0minput_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0moutput_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_image\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0moutput_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_meta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPFail\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_fallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgument\u001b[0m: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: input for the following indices\n index: 1 Got: 3 Expected: 32\n index: 3 Got: 32 Expected: 3\n Please fix either the inputs/outputs or the model."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Identification"
      ],
      "metadata": {
        "id": "FoYQISFFtqv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install tesseract-ocr\n",
        "!pip install pytesseract"
      ],
      "metadata": {
        "id": "S_MFTuJytxRZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f694782c-ae4c-44c1-bbb7-d1d520ef5539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 3s (1,874 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 121918 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.0)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pytesseract\n",
        "\n",
        "# Path to your input image in Google Colab\n",
        "input_image_path = '/content/WhatsApp Image 2024-05-16 at 12.41.25_ebd49ab3.jpg'\n",
        "\n",
        "# Load the input image using OpenCV\n",
        "image = cv2.imread(input_image_path)\n",
        "\n",
        "# Convert the image to grayscale\n",
        "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Use Tesseract with the LSTM OCR engine to perform OCR on the image\n",
        "recognized_text = pytesseract.image_to_string(gray_image, config='--oem 1 --psm 6')\n",
        "\n",
        "# Print the recognized text\n",
        "print(recognized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deJuvdSWrx0D",
        "outputId": "06a6734a-04ef-40af-e2c9-8df6f05e3162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Program status Register .\n",
            "Capy 0 ey ano coe internal Operations.\n",
            "\f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWbbwm-4t0dK",
        "outputId": "9cbba930-d66c-4b96-d481-8f2d6f63811f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.2)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.0.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNjW9v960v5E",
        "outputId": "e381c82e-8bd6-4ad1-f626-3f964b370106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGcBANtB1_Pg",
        "outputId": "7c2da65a-bdd1-49c8-d2b1-650b8b34ade2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Format the input text as a question\n",
        "input_text = \"Are cats and dogs from the same family?\"\n",
        "question = f\"Is it true that {input_text}?\"\n",
        "\n",
        "# Tokenize the input text and convert it to tensors\n",
        "input_ids = torch.tensor([tokenizer.encode(question, add_special_tokens=True)]).cuda()\n",
        "\n",
        "# Get the model's prediction\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
        "    predicted_class = torch.argmax(probabilities, dim=1)\n",
        "\n",
        "# Map the predicted class to a true/false label\n",
        "label_map = {0: \"False\", 1: \"True\"}\n",
        "label = label_map[predicted_class.item()]\n",
        "\n",
        "# Print the result\n",
        "print(f\"The statement '{input_text}' is {label}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "-4CFFarv0zv2",
        "outputId": "6b38d177-4b02-42ef-a3ed-d7dd77212979"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-f6b5261ffb89>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Tokenize the input text and convert it to tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Get the model's prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    }
  ]
}